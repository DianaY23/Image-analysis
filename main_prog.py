# -*- coding: utf-8 -*-
"""main_prog.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gPkTaKvor6ft1zAWdkkv0a1YDJI-WKUC
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install petroscope

from pathlib import Path
from petroscope.segmentation.classes import ClassSet, LumenStoneClasses
from petroscope.segmentation.utils import load_image, load_mask
from petroscope.segmentation import GeoSegmModel
import numpy as np
from tqdm import tqdm
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from skimage.color import rgb2lab

ds_path = Path('/content/drive/MyDrive/PhotoSets/')

classset = LumenStoneClasses.S1v1()

for cl in classset.classes:
    print(cl)

train_img_mask_p = [
    (img_p, ds_path / "masks" / "train" / f"{img_p.stem}.png")
    for img_p in sorted((ds_path / "imgs" / "train").iterdir())
]

test_img_mask_p = [
    (img_p, ds_path / "masks" / "test" / f"{img_p.stem}.png")
    for img_p in sorted((ds_path / "imgs" / "test").iterdir())
]

for img_p, _ in train_img_mask_p:
    img = load_image(img_p, normalize=True)
    img_lab = rgb2lab(img)
    print(f"Image {img_p.name}: {img_lab.shape}, {img_lab.dtype}")

for _, mask_p in train_img_mask_p:
    mask = load_mask(mask_p, classes=classset, one_hot=False)
    print(f"Mask {mask_p.name}: {mask.shape}, {mask.dtype}")

for _, mask_p in train_img_mask_p:
    mask_one_hot = load_mask(mask_p, classes=classset, one_hot=True)
    print(f"Mask one-hot {mask_p.name}: {mask_one_hot.shape}, {mask_one_hot.dtype}")

for img_p, _ in train_img_mask_p:
    mask_colored_path = ds_path / "masks_colored_png" / "train" / f"{img_p.stem}.png"
    mask_colored = load_image(mask_colored_path, normalize=False)
    print(f"Colored mask {mask_colored_path.name}: {mask_colored.shape}, {mask_colored.dtype}")

def load_and_preprocess(img_path, mask_path, classes, img_size=(256, 256)):
    img = load_image(img_path, normalize=True)
    img = tf.image.resize(img, img_size)

    mask = load_mask(mask_path, classes=classes, one_hot=False)
    mask = tf.image.resize(mask[..., np.newaxis], img_size, method='nearest')
    mask = to_categorical(mask, num_classes=len(classes))

    return img, mask

def unet_model(input_size=(256, 256, 3), num_classes=7):
    inputs = Input(input_size)

    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)
    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)

    up5 = concatenate([UpSampling2D(size=(2, 2))(conv4), conv3], axis=-1)
    conv5 = Conv2D(256, 3, activation='relu', padding='same')(up5)
    conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)
    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv2], axis=-1)
    conv6 = Conv2D(128, 3, activation='relu', padding='same')(up6)
    conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)
    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv1], axis=-1)
    conv7 = Conv2D(64, 3, activation='relu', padding='same')(up7)
    conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)

    outputs = Conv2D(num_classes, 1, activation='softmax')(conv7)

    model = Model(inputs=[inputs], outputs=[outputs])
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

train_data = [load_and_preprocess(img_p, mask_p, classset) for img_p, mask_p in train_img_mask_p]
train_images, train_masks = zip(*train_data)
train_images = np.array(train_images)
train_masks = np.array(train_masks)

print(f"Train images shape: {train_images.shape}")
print(f"Train masks shape: {train_masks.shape}")

model = unet_model(num_classes=len(classset.classes))

model.fit([train_images], [train_masks], batch_size=4, epochs=200, validation_split=0.1)

from petroscope.segmentation.eval import SegmDetailedTester
from tensorflow.keras.utils import to_categorical

tester = SegmDetailedTester(
    Path("output"),
    classes=classset,
    void_pad=0,
    void_border_width=4,
    vis_plots=False,
    vis_segmentation=True,
)

for img_p, mask_p in test_img_mask_p:
    img = load_image(img_p, normalize=True)
    img_resized = tf.image.resize(img, (256, 256))

    pred = model.predict(np.expand_dims(img_resized, axis=0))
    pred = np.argmax(pred[0], axis=-1)
    pred = pred.astype(np.uint8)
    print(f"Pred shape: {pred.shape}")

    mask = load_mask(mask_p, classes=classset, one_hot=False)
    mask_resized = tf.image.resize(mask[..., np.newaxis], (256, 256), method='nearest')
    mask_resized = mask_resized[..., 0].numpy().astype(np.uint8)
    print(f"Mask resized shape: {mask_resized.shape}")

    pred_one_hot = to_categorical(pred, num_classes=len(classset.classes))
    mask_one_hot = to_categorical(mask_resized, num_classes=len(classset.classes))
    print(f"Pred one-hot shape: {pred_one_hot.shape}")
    print(f"Mask one-hot shape: {mask_one_hot.shape}")

    metrics = tester.eval.evaluate(pred_one_hot, gt=mask_one_hot)
    metrics_void = tester.eval_void.evaluate(pred_one_hot, gt=mask_one_hot)

    print(f"Metrics for {img_p.name}:\n{metrics}")
    print(f"Metrics with void borders for {img_p.name}:\n{metrics_void}")
    print("-" * 50)