# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_Fh04m-F-vEu6mBQDIkwk2PFmJAP1tC

Подключение google disk для использования dataset
"""

from google.colab import drive
drive.mount('/content/drive')

"""Установка библиотеки"""

!pip install petroscope

"""Импорт неободимых библиотек"""

from pathlib import Path
from petroscope.segmentation.classes import ClassSet, LumenStoneClasses
from petroscope.segmentation.utils import load_image, load_mask
from petroscope.segmentation import GeoSegmModel
import numpy as np
from tqdm import tqdm
import tensorflow as tf
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from skimage.color import rgb2lab
import os

"""Путь к dataset"""

ds_path = Path('/content/drive/MyDrive/PhotoSets/')

"""Инциализация набора классов для сегментации"""

classset = LumenStoneClasses.S1v1()

for cl in classset.classes:
    print(cl)

"""Формирование путей к изображениям и маскам"""

train_img_mask_p = [
    (img_p, ds_path / "masks" / "train" / f"{img_p.stem}.png")
    for img_p in sorted((ds_path / "imgs" / "train").iterdir())
]

test_img_mask_p = [
    (img_p, ds_path / "masks" / "test" / f"{img_p.stem}.png")
    for img_p in sorted((ds_path / "imgs" / "test").iterdir())
]

"""Загрузка и конвертация обучающих изображений в пространство LAB"""

for img_p, _ in train_img_mask_p:
    img = load_image(img_p, normalize=True)
    img_lab = rgb2lab(img)
    print(f"Image {img_p.name}: {img_lab.shape}, {img_lab.dtype}")

"""Загрузка масок без one-hot кодирования"""

for _, mask_p in train_img_mask_p:
    mask = load_mask(mask_p, classes=classset, one_hot=False)
    print(f"Mask {mask_p.name}: {mask.shape}, {mask.dtype}")

"""Загрузка масок с one-hot кодированием"""

for _, mask_p in train_img_mask_p:
    mask_one_hot = load_mask(mask_p, classes=classset, one_hot=True)
    print(f"Mask one-hot {mask_p.name}: {mask_one_hot.shape}, {mask_one_hot.dtype}")

"""Загрузка цветных масок"""

for img_p, _ in train_img_mask_p:
    mask_colored_path = ds_path / "masks_colored_png" / "train" / f"{img_p.stem}.png"
    mask_colored = load_image(mask_colored_path, normalize=False)
    print(f"Colored mask {mask_colored_path.name}: {mask_colored.shape}, {mask_colored.dtype}")

"""Функция для загрузки и предобработки изображений и масок"""

def load_and_preprocess(img_path, mask_path, classes, img_size=(256, 256)):
    img = load_image(img_path, normalize=True)
    img = tf.image.resize(img, img_size)

    mask = load_mask(mask_path, classes=classes, one_hot=False)
    mask = tf.image.resize(mask[..., np.newaxis], img_size, method='nearest')
    mask = to_categorical(mask, num_classes=len(classes))

    return img, mask

"""Унаследованный класс от GeoSegmModel, метод сегментации - нейронная сеть U-Net"""

class UNetSegmModel(GeoSegmModel):
    def __init__(self, classes: ClassSet, input_size=(256, 256, 3), save_path="unet_model"):
        super().__init__()
        self.classes = classes
        self.input_size = input_size
        self.save_path = Path(save_path)
        self.model = self._build_model()

    def _build_model(self):
        inputs = Input(self.input_size)

        conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
        conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)
        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
        conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
        conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)
        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
        conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)
        conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)
        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
        conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)
        conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)

        up5 = concatenate([UpSampling2D(size=(2, 2))(conv4), conv3], axis=-1)
        conv5 = Conv2D(256, 3, activation='relu', padding='same')(up5)
        conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)
        up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv2], axis=-1)
        conv6 = Conv2D(128, 3, activation='relu', padding='same')(up6)
        conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)
        up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv1], axis=-1)
        conv7 = Conv2D(64, 3, activation='relu', padding='same')(up7)
        conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)

        outputs = Conv2D(len(self.classes), 1, activation='softmax')(conv7)

        model = Model(inputs=[inputs], outputs=[outputs])
        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
        return model

    def train(self, img_mask_paths, epochs=20, batch_size=4, validation_split=0.1, **kwargs):
        train_data = [load_and_preprocess(img_p, mask_p, self.classes) for img_p, mask_p in tqdm(img_mask_paths, desc="Loading data")]
        train_images, train_masks = zip(*train_data)
        train_images = np.array(train_images)
        train_masks = np.array(train_masks)

        self.model.fit([train_images], [train_masks], batch_size=batch_size, epochs=epochs, validation_split=validation_split)

        self.save()

    def predict_image(self, image: np.ndarray) -> np.ndarray:
        img_resized = tf.image.resize(image, self.input_size[:2])
        pred = self.model.predict(np.expand_dims(img_resized, axis=0))
        pred = np.argmax(pred[0], axis=-1)
        return pred.astype(np.uint8)

    def save(self):
        self.save_path.mkdir(parents=True, exist_ok=True)
        self.model.save(self.save_path / "unet_model.keras")

    def load(self, saved_path: Path, **kwargs):
        if not saved_path.exists():
            raise FileNotFoundError(f"Model file not found at {saved_path}")
        self.model = load_model(saved_path / "unet_model.keras")

"""Обучение модели"""

model = UNetSegmModel(classes=classset, input_size=(256, 256, 3), save_path="unet_model")

model.train(train_img_mask_p, epochs=200, batch_size=8, validation_split=0.1)

"""Тестирование"""

from petroscope.segmentation.eval import SegmDetailedTester
from tensorflow.keras.utils import to_categorical

tester = SegmDetailedTester(
    Path("output"),
    classes=classset,
    void_pad=0,
    void_border_width=4,
    vis_plots=False,
    vis_segmentation=True,
)

for img_p, mask_p in test_img_mask_p:
    img = load_image(img_p, normalize=True)

    pred = model.predict_image(img)
    print(f"Pred shape: {pred.shape}")

    mask = load_mask(mask_p, classes=classset, one_hot=False)
    mask_resized = tf.image.resize(mask[..., np.newaxis], (256, 256), method='nearest')
    mask_resized = mask_resized[..., 0].numpy().astype(np.uint8)
    print(f"Mask resized shape: {mask_resized.shape}")

    pred_one_hot = to_categorical(pred, num_classes=len(classset.classes))
    mask_one_hot = to_categorical(mask_resized, num_classes=len(classset.classes))
    print(f"Pred one-hot shape: {pred_one_hot.shape}")
    print(f"Mask one-hot shape: {mask_one_hot.shape}")

    metrics = tester.eval.evaluate(pred_one_hot, gt=mask_one_hot)
    metrics_void = tester.eval_void.evaluate(pred_one_hot, gt=mask_one_hot)

    print(f"Metrics for {img_p.name}:\n{metrics}")
    print(f"Metrics with void borders for {img_p.name}:\n{metrics_void}")
    print("-" * 50)